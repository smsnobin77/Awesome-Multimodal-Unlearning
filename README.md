<h1 align="center">
  Multimodal Unlearning Across Vision, Language, Video, and Audio:
  Survey of Methods, Datasets, and Benchmarks
</h1>

<div align="center">
  <a href="https://doi.org/10.36227/techrxiv.176945748.88280394/v1">
    <img src="https://img.shields.io/badge/DOI-TechRxiv-blue?logo=ieee" alt="DOI">
  </a>
  <a href="https://github.com/smsnobin77/Awesome-Multimodal-Unlearning/pulls">
    <img src="https://img.shields.io/badge/PRs-welcome-brightgreen.svg" alt="PRs Welcome">
  </a>
  <a href="https://github.com/smsnobin77/Awesome-Multimodal-Unlearning">
    <img src="https://img.shields.io/github/stars/smsnobin77/Awesome-Multimodal-Unlearning?style=social" alt="GitHub Stars">
  </a>
</div>

> **Authors**: Nobin Sarwar<sup>ğŸ«</sup>, Shubhashis Roy Dipta<sup>ğŸ«</sup>, Zheyuan Liu<sup>ğŸ“</sup>, Vaidehi Patil<sup>ğŸ›ï¸</sup>  
>
> **Affiliations**: <sup>ğŸ«</sup> University of Maryland, Baltimore County &nbsp;Â·&nbsp; <sup>ğŸ“</sup> University of Notre Dame &nbsp;Â·&nbsp; <sup>ğŸ›ï¸</sup> UNC Chapel Hill


## ğŸ§¾ Citation

If our work supports your research or applications, we would appreciate a â­ and a citation using the BibTeX below.

```bibtex
@article{sarwar2026mm-unlearning-survey,
  title = {{Multimodal Unlearning Across Vision, Language, Video, and Audio: Survey of Methods, Datasets, and Benchmarks}},
  author = {Sarwar, Nobin and Roy Dipta, Shubhashis and Liu, Zheyuan and Patil, Vaidehi},
  year = {2026},
  doi = {10.36227/techrxiv.176945748.88280394/v1},
  url = {https://doi.org/10.36227/techrxiv.176945748.88280394/v1},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  month = jan
}
```

## âœ‰ï¸ Contact

This repository is actively maintained and continuously updated ğŸš€. If you notice any issues or would like your work on Multimodal Unlearning included, please open an issue or contact us via email.

**Corresponding author:** Nobin Sarwar (smsarwar96@gmail.com)
